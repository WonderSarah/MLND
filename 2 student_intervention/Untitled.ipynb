{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project 2: Supervised Learning\n",
    "## Building a Student Intervention System\n",
    "\n",
    "### Install\n",
    "\n",
    "This project requires **Python 2.7** and the following Python libraries installed:\n",
    "\n",
    "- [NumPy](http://www.numpy.org/)\n",
    "- [Pandas](http://pandas.pydata.org)\n",
    "- [scikit-learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "You will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n",
    "\n",
    "Udacity recommends our students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n",
    "\n",
    "### Code\n",
    "\n",
    "Template code is provided in the notebook `student_intervention.ipynb` notebook file. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n",
    "\n",
    "### Run\n",
    "\n",
    "In a terminal or command window, navigate to the top-level project directory `student_intervention/` (that contains this README) and run one of the following commands:\n",
    "\n",
    "```ipython notebook student_intervention.ipynb```  \n",
    "```jupyter notebook student_intervention.ipynb```\n",
    "\n",
    "This will open the iPython Notebook software and project file in your browser.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset used in this project is included as `student-data.csv`. This dataset has the following attributes:\n",
    "\n",
    "- `school` : student's school (binary: \"GP\" or \"MS\")\n",
    "- `sex` : student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "- `age` : student's age (numeric: from 15 to 22)\n",
    "- `address` : student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "- `famsize` : family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "- `Pstatus` : parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "- `Medu` : mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n",
    "- `Fedu` : father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n",
    "- `Mjob` : mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "- `Fjob` : father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "- `reason` : reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n",
    "- `guardian` : student's guardian (nominal: \"mother\", \"father\" or \"other\")\n",
    "- `traveltime` : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "- `studytime` : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "- `failures` : number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "- `schoolsup` : extra educational support (binary: yes or no)\n",
    "- `famsup` : family educational support (binary: yes or no)\n",
    "- `paid` : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "- `activities` : extra-curricular activities (binary: yes or no)\n",
    "- `nursery` : attended nursery school (binary: yes or no)\n",
    "- `higher` : wants to take higher education (binary: yes or no)\n",
    "- `internet` : Internet access at home (binary: yes or no)\n",
    "- `romantic` : with a romantic relationship (binary: yes or no)\n",
    "- `famrel` : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "- `freetime` : free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "- `goout` : going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "- `Dalc` : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- `Walc` : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- `health` : current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "- `absences` : number of school absences (numeric: from 0 to 93)\n",
    "- `passed` : did the student pass the final exam (binary: yes or no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Study\\\\Data Analytics\\\\_Courses\\\\Udacity-MLND\\\\machine-learning-master-finished\\\\projects\\\\student_intervention'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print (\"Student data read successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "395\n",
      "0\n",
      "Total number of students: 395\n",
      "Number of features: 30\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate number of students\n",
    "n_students = len(student_data)\n",
    "n_students = student_data.shape[0]\n",
    "\n",
    "# TODO: Calculate number of features\n",
    "n_features = len(student_data.columns)-1\n",
    "n_features = student_data.shape[1]-1\n",
    "\n",
    "# TODO: Calculate passing students\n",
    "n_passed = len(student_data.loc[student_data['passed'] == 'yes'])\n",
    "n_passed = len(student_data[['passed']].query('passed==\"yes\"'))\n",
    "n_passed = len(student_data[student_data.passed=='yes'])\n",
    "\n",
    "# TODO: Calculate failing students\n",
    "n_failed = len(student_data.loc[student_data['passed'] == 'no'])\n",
    "n_failed = len(student_data[['passed']].query('passed==\"no\"'))\n",
    "n_failed = len(student_data[student_data.passed=='no'])\n",
    "\n",
    "# TODO: Calculate graduation rate\n",
    "grad_rate = 0.0\n",
    "grad_rate = np.divide(float(n_passed),n_students)*100\n",
    "\n",
    "\n",
    "print n_passed\n",
    "print n_students\n",
    "print np.divide(n_passed,n_students)\n",
    "\n",
    "# Print the results\n",
    "print (\"Total number of students: {}\".format(n_students))\n",
    "print (\"Number of features: {}\".format(n_features))\n",
    "print (\"Number of students who passed: {}\".format(n_passed))\n",
    "print (\"Number of students who failed: {}\".format(n_failed))\n",
    "print (\"Graduation rate of the class: {:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "\n",
      "Target column: passed\n",
      "\n",
      "Feature values:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns\n",
    "feature_cols = list(student_data.columns[:-1])\n",
    "\n",
    "# Extract target column 'passed'\n",
    "target_col = student_data.columns[-1] \n",
    "\n",
    "# Show the list of columns\n",
    "print (\"Feature columns:\\n{}\".format(feature_cols))\n",
    "print (\"\\nTarget column: {}\".format(target_col))\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = student_data[feature_cols]\n",
    "y_all = student_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print (\"\\nFeature values:\")\n",
    "print (X_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48 total features):\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the student data and converts non-numeric binary variables into\n",
    "        binary (0/1) variables. Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print (\"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 300 samples.\n",
      "Testing set has 95 samples.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import any additional functionality you may need here\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# TODO: Set the number of training points\n",
    "num_train = 300\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, test_size=num_test, random_state=0)\n",
    "\n",
    "#X_train = None\n",
    "#X_test = None\n",
    "#y_train = None\n",
    "#y_test = None\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     school_GP  school_MS  sex_F  sex_M  age  address_R  address_U  \\\n",
      "63         1.0        0.0    1.0    0.0   16        0.0        1.0   \n",
      "245        1.0        0.0    0.0    1.0   16        0.0        1.0   \n",
      "154        1.0        0.0    1.0    0.0   17        0.0        1.0   \n",
      "311        1.0        0.0    1.0    0.0   19        0.0        1.0   \n",
      "81         1.0        0.0    0.0    1.0   15        0.0        1.0   \n",
      "\n",
      "     famsize_GT3  famsize_LE3  Pstatus_A    ...     higher  internet  \\\n",
      "63           1.0          0.0        0.0    ...          1         1   \n",
      "245          1.0          0.0        0.0    ...          1         1   \n",
      "154          1.0          0.0        0.0    ...          1         0   \n",
      "311          1.0          0.0        0.0    ...          0         1   \n",
      "81           1.0          0.0        0.0    ...          1         1   \n",
      "\n",
      "     romantic  famrel  freetime  goout  Dalc  Walc  health  absences  \n",
      "63          0       3         4      4     2     4       4         2  \n",
      "245         0       4         3      3     1     1       4         6  \n",
      "154         1       4         2      1     1     1       4         0  \n",
      "311         1       3         4      1     1     1       2        20  \n",
      "81          0       5         3      2     1     2       5         4  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "print (X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    #print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    #print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"{} F1 score for test set: {:.4f}.\".format(clf.__class__.__name__,predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 100 samples.\n",
      "Testing set has 295 samples.\n",
      "Training set has 200 samples.\n",
      "Testing set has 195 samples.\n",
      "Training set has 300 samples.\n",
      "Testing set has 95 samples.\n"
     ]
    }
   ],
   "source": [
    "X_train_100, X_test_100, y_train_100, y_test_100 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-100, random_state=0)\n",
    "#X_train_100 = None\n",
    "#y_train_100 = None\n",
    "print (\"Training set has {} samples.\".format(X_train_100.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test_100.shape[0]))\n",
    "\n",
    "X_train_200, X_test_200, y_train_200, y_test_200 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-200, random_state=0)\n",
    "#X_train_200 = None\n",
    "#y_train_200 = None\n",
    "print (\"Training set has {} samples.\".format(X_train_200.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test_200.shape[0]))\n",
    "\n",
    "X_train_300, X_test_300, y_train_300, y_test_300 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-300, random_state=0)\n",
    "#X_train_300 = None\n",
    "#y_train_300 = None\n",
    "print (\"Training set has {} samples.\".format(X_train_300.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test_300.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 100 samples.\n",
      "Testing set has 100 samples.\n",
      "Training set has 200 samples.\n",
      "Testing set has 200 samples.\n",
      "Training set has 300 samples.\n",
      "Testing set has 300 samples.\n"
     ]
    }
   ],
   "source": [
    "X_train_100 = X_train[0:100]\n",
    "y_train_100 = y_train[0:100]\n",
    "print (\"Training set has {} samples.\".format(X_train_100.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(y_train_100.shape[0]))\n",
    "X_train_200 = X_train[0:200]\n",
    "y_train_200 = y_train[0:200]\n",
    "print (\"Training set has {} samples.\".format(X_train_200.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(y_train_200.shape[0]))\n",
    "X_train_300 = X_train\n",
    "y_train_300 = y_train\n",
    "print (\"Training set has {} samples.\".format(X_train_300.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(y_train_300.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0030 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7119.\n",
      "Training a LogisticRegression using a training set size of 300. . .\n",
      "Trained model in 0.0050 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8381.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7910.\n",
      "Training a SVC using a training set size of 300. . .\n",
      "Trained model in 0.0110 seconds\n",
      "Made predictions in 0.0090 seconds.\n",
      "F1 score for training set: 0.8692.\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7586.\n",
      "Training a GaussianNB using a training set size of 300. . .\n",
      "Trained model in 0.0020 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8088.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7500.\n",
      "Training a KNeighborsClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0160 seconds.\n",
      "F1 score for training set: 0.8722.\n",
      "Made predictions in 0.0050 seconds.\n",
      "F1 score for test set: 0.7482.\n",
      "Training a SGDClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.1000.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.1250.\n",
      "Training a BaggingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0370 seconds\n",
      "Made predictions in 0.2010 seconds.\n",
      "F1 score for training set: 0.8705.\n",
      "Made predictions in 0.0570 seconds.\n",
      "F1 score for test set: 0.7068.\n",
      "Training a BaggingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0580 seconds\n",
      "Made predictions in 0.0100 seconds.\n",
      "F1 score for training set: 0.7826.\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7500.\n",
      "Training a RandomForestClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0470 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for training set: 0.9903.\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7077.\n",
      "Training a ExtraTreesClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0550 seconds\n",
      "Made predictions in 0.0050 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7612.\n",
      "Training a GradientBoostingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.2140 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.9740.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7727.\n",
      "Training a AdaBoostClassifier using a training set size of 300. . .\n",
      "Trained model in 0.2470 seconds\n",
      "Made predictions in 0.0090 seconds.\n",
      "F1 score for training set: 0.8688.\n",
      "Made predictions in 0.0060 seconds.\n",
      "F1 score for test set: 0.7794.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "\n",
    "clf_A = tree.DecisionTreeClassifier()\n",
    "clf_B = linear_model.LogisticRegression()\n",
    "clf_C = svm.SVC()\n",
    "clf_D = naive_bayes.GaussianNB() \n",
    "clf_E = neighbors.KNeighborsClassifier()\n",
    "clf_F = linear_model.SGDClassifier()\n",
    "clf_G1 = BaggingClassifier(KNeighborsClassifier())\n",
    "clf_G2 = BaggingClassifier(linear_model.SGDClassifier())\n",
    "clf_H = RandomForestClassifier()\n",
    "clf_I = ExtraTreesClassifier()\n",
    "clf_J = GradientBoostingClassifier()\n",
    "clf_K = AdaBoostClassifier()\n",
    "#clf_L = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "#X_train_100, X_test_100, y_train_100, y_test_100 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-100, random_state=0)\n",
    "#X_train_100 = None\n",
    "#y_train_100 = None\n",
    "\n",
    "#X_train_200, X_test_200, y_train_200, y_test_200 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-200, random_state=0)\n",
    "#X_train_200 = None\n",
    "#y_train_200 = None\n",
    "\n",
    "#X_train_300, X_test_300, y_train_300, y_test_300 = cross_validation.train_test_split(X_all, y_all, test_size=X_all.shape[0]-300, random_state=0)\n",
    "#X_train_300 = None\n",
    "#y_train_300 = None\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "train_predict(clf_A, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_B, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_C, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_D, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_E, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_F, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_G1, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_G2, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_H, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_I, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_J, X_train_300, y_train_300, X_test, y_test)\n",
    "\n",
    "train_predict(clf_K, X_train_300, y_train_300, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 0.0040 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7692.\n",
      "Trained model in 0.0040 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.6718.\n",
      "Trained model in 0.0020 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7273.\n",
      "Trained model in 0.0080 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7647.\n",
      "Trained model in 0.0080 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7549.\n",
      "Trained model in 0.0160 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.6176.\n",
      "Trained model in 0.0100 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7586.\n",
      "Trained model in 0.0060 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7781.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7824.\n",
      "Trained model in 0.0020 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7500.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7331.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.3636.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.7482.\n",
      "Trained model in 0.0020 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.7639.\n",
      "Trained model in 0.0000 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.7780.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7794.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7881.\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7564.\n",
      "Trained model in 0.0280 seconds\n",
      "Made predictions in 0.0120 seconds.\n",
      "F1 score for test set: 0.7550.\n",
      "Trained model in 0.0220 seconds\n",
      "Made predictions in 0.0190 seconds.\n",
      "F1 score for test set: 0.7846.\n",
      "Trained model in 0.0340 seconds\n",
      "Made predictions in 0.0190 seconds.\n",
      "F1 score for test set: 0.7851.\n",
      "Trained model in 0.0620 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7970.\n",
      "Trained model in 0.0560 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.5972.\n",
      "Trained model in 0.0400 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.7682.\n",
      "Trained model in 0.0350 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7344.\n",
      "Trained model in 0.0540 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7473.\n",
      "Trained model in 0.0440 seconds\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.6976.\n",
      "Trained model in 0.0440 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7429.\n",
      "Trained model in 0.0270 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7653.\n",
      "Trained model in 0.0290 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7422.\n",
      "Trained model in 0.0810 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7761.\n",
      "Trained model in 0.0560 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7433.\n",
      "Trained model in 0.0270 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.6911.\n",
      "Trained model in 0.3340 seconds\n",
      "Made predictions in 0.0090 seconds.\n",
      "F1 score for test set: 0.7820.\n",
      "Trained model in 0.2280 seconds\n",
      "Made predictions in 0.0140 seconds.\n",
      "F1 score for test set: 0.7433.\n",
      "Trained model in 0.3370 seconds\n",
      "Made predictions in 0.0140 seconds.\n",
      "F1 score for test set: 0.7121.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "\n",
    "clf_A = tree.DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "clf_B = linear_model.LogisticRegression(C=1e5)\n",
    "clf_C = svm.SVC(kernel='rbf')\n",
    "clf_D = naive_bayes.GaussianNB() \n",
    "clf_E = neighbors.KNeighborsClassifier()\n",
    "clf_F = linear_model.SGDClassifier()\n",
    "clf_G1 = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "clf_G2 = BaggingClassifier(linear_model.SGDClassifier(), max_samples=0.5, max_features=0.5)\n",
    "clf_H = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
    "clf_I = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
    "clf_J = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf_K = AdaBoostClassifier(n_estimators=100)\n",
    "#clf_L = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "X_train_100 = X_train[0:100]\n",
    "y_train_100 = y_train[0:100]\n",
    "\n",
    "X_train_200 = X_train[0:200]\n",
    "y_train_200 = y_train[0:200]\n",
    "\n",
    "X_train_300 = X_train\n",
    "y_train_300 = y_train\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "train_predict(clf_A, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_A, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_A, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_B, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_B, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_B, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_C, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_C, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_C, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_D, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_D, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_D, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_E, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_E, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_E, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_F, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_F, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_F, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_G1, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_G1, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_G1, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_G2, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_G2, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_G2, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_H, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_H, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_H, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_I, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_I, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_I, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_J, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_J, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_J, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "train_predict(clf_K, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "train_predict(clf_K, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "train_predict(clf_K, X_train_300, y_train_300, X_test_300, y_test_300)\n",
    "\n",
    "#train_predict(clf_L, X_train_100, y_train_100, X_test_100, y_test_100)\n",
    "#train_predict(clf_L, X_train_200, y_train_200, X_test_200, y_test_200)\n",
    "#train_predict(clf_L, X_train_300, y_train_300, X_test_300, y_test_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0030 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8295.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7852.\n"
     ]
    }
   ],
   "source": [
    "#tuning decisiontree\n",
    "clf_A = tree.DecisionTreeClassifier(criterion='gini',max_leaf_nodes=4, max_depth=1, random_state=0)\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0020 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8270.\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for test set: 0.7971.\n"
     ]
    }
   ],
   "source": [
    "clf_A = tree.DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=3, max_depth=2, random_state=0)\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a training F1 score of 1.0000.\n",
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a testing F1 score of 0.7692.\n",
      "Parameter 'criterion' is entropy for the optimal model.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'criterion': ['gini','entropy']}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'criterion' is {} for the optimal model.\".format(clf.get_params()['criterion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a training F1 score of 0.8326.\n",
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a testing F1 score of 0.7941.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'max_depth': np.arange(1, 21)}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "#f1_score(target.values, y_pred, pos_label='yes')\n",
    "#Create the F1 scoring function using make_scorer and store it in f1_scorer.\n",
    "#Set the pos_label parameter to the correct value!\n",
    "#clf.fit(X_train, y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "#print (\"Parameter 'max_depth' is {} for the optimal model.\".format(clf.get_params()['max_depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a training F1 score of 0.8270.\n",
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a testing F1 score of 0.7971.\n",
      "Parameter 'max_leaf_nodes' is 3 for the optimal model.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'max_leaf_nodes': np.arange(2, 21)}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'max_leaf_nodes' is {} for the optimal model.\".format(clf.get_params()['max_leaf_nodes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a training F1 score of 0.8295.\n",
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a testing F1 score of 0.7852.\n",
      "Parameter 'criterion' is gini for the optimal model.\n",
      "{'max_leaf_nodes': 4, 'criterion': 'gini', 'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "param_range= np.arange(1, 21)\n",
    "parameters = [{'criterion': ['gini'], 'max_depth': param_range, 'max_leaf_nodes': param_range[1:]},\n",
    "              {'criterion': ['entropy'], 'max_depth': param_range, 'max_leaf_nodes': param_range[1:]},\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'criterion' is {} for the optimal model.\".format(clf.get_params()['criterion']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a SVC using a training set size of 300. . .\n",
      "Trained model in 0.0150 seconds\n",
      "Made predictions in 0.0090 seconds.\n",
      "F1 score for training set: 0.9717.\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for test set: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "#tuning SVM\n",
    "clf_A = svm.SVC(kernel='rbf', C=1, gamma=0.1, decision_function_shape='ovo')\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0100 seconds.\n",
      "Tuned model has a training F1 score of 0.9717.\n",
      "Made predictions in 0.0030 seconds.\n",
      "Tuned model has a testing F1 score of 0.7919.\n",
      "Parameter 'kernel' is rbf for the optimal model.\n",
      "{'kernel': 'rbf', 'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "param_range= [1e-2,1e-1,1,1e1,1e2]\n",
    "degree_range= [2,3,4]\n",
    "parameters = [{'kernel': ['linear'],'C':param_range, 'decision_function_shape':['ovo','ovr','None']},\n",
    "              {'kernel': ['rbf'],'C':param_range,'gamma':param_range, 'decision_function_shape':['ovo','ovr','None']},\n",
    "              {'kernel': ['sigmoid'],'C':param_range,'gamma':param_range, 'decision_function_shape':['ovo','ovr','None']},\n",
    "              {'kernel': ['poly'],'degree':degree_range,'gamma':param_range, 'decision_function_shape':['ovo','ovr','None']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = svm.SVC(random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'kernel' is {} for the optimal model.\".format(clf.get_params()['kernel']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LogisticRegression using a training set size of 300. . .\n",
      "Trained model in 0.0050 seconds\n",
      "Made predictions in 0.0000 seconds.\n",
      "F1 score for training set: 0.8326.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7857.\n"
     ]
    }
   ],
   "source": [
    "#tuning LogisticRegression\n",
    "clf_A = linear_model.LogisticRegression(penalty='l2', C=0.1, tol=1e-5, solver='liblinear')\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0120 seconds.\n",
      "Tuned model has a training F1 score of 0.8267.\n",
      "Made predictions in 0.0140 seconds.\n",
      "Tuned model has a testing F1 score of 0.7917.\n",
      "Parameter 'penalty' is l1 for the optimal model.\n",
      "{'penalty': 'l1', 'C': 0.1, 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "param_range= [1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3,1e4,1e5]\n",
    "parameters = [{'penalty': ['l1'],'C':param_range, 'tol':param_range},\n",
    "              {'penalty': ['l2'],'C':param_range, 'tol':param_range},\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = linear_model.LogisticRegression(random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'penalty' is {} for the optimal model.\".format(clf.get_params()['penalty']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is not binary and pos_label is not specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-483e9a763ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0maverage_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"yes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0maverage_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36maverage_precision_score\u001b[1;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     return _average_binary_score(_binary_average_precision, y_true, y_score,\n\u001b[1;32m--> 182\u001b[1;33m                                  average, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\base.pyc\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36m_binary_average_precision\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_binary_average_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         precision, recall, thresholds = precision_recall_curve(\n\u001b[1;32m--> 178\u001b[1;33m             y_true, y_score, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[1;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    407\u001b[0m     fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,\n\u001b[0;32m    408\u001b[0m                                              \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                                              sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtps\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    306\u001b[0m              \u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m              array_equal(classes, [1]))):\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data is not binary and pos_label is not specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data is not binary and pos_label is not specified"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', C=0.1, tol=1e-5, solver='liblinear')\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute Precision-Recall and plot curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(y_test,y_score, pos_label=\"yes\")\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.clf()\n",
    "plt.plot(recall[0], precision[0], label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0]))\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a SGDClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0220 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.8194.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7826.\n"
     ]
    }
   ],
   "source": [
    "#tuning SGDClassifier\n",
    "clf_A = linear_model.SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=100)\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0000 seconds.\n",
      "Tuned model has a training F1 score of 0.8269.\n",
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a testing F1 score of 0.7838.\n",
      "Parameter 'loss' is hinge for the optimal model.\n",
      "{'penalty': 'l1', 'alpha': 0.01, 'n_iter': 100.0, 'loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "param_range= [1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3,1e4,1e5]\n",
    "parameters = [{'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], 'penalty': ['l1','l2','elasticnet'],'alpha':param_range, 'n_iter':param_range[5:8]}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = linear_model.SGDClassifier(random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'loss' is {} for the optimal model.\".format(clf.get_params()['loss']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a KNeighborsClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0050 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0020 seconds.\n",
      "F1 score for test set: 0.7482.\n"
     ]
    }
   ],
   "source": [
    "#tuning KNN\n",
    "clf_A = neighbors.KNeighborsClassifier(n_neighbors=8, weights='distance', algorithm='brute')\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0050 seconds.\n",
      "Tuned model has a training F1 score of 1.0000.\n",
      "Made predictions in 0.0030 seconds.\n",
      "Tuned model has a testing F1 score of 0.7482.\n",
      "Parameter 'n_neighbors' is 8 for the optimal model.\n",
      "{'n_neighbors': 8, 'weights': 'distance', 'algorithm': 'brute'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "neighbors_range= [1,2,3,4,5,6,7,8,9,10]\n",
    "parameters = [{'n_neighbors':neighbors_range, 'weights': ['uniform','distance'],'algorithm':['auto','ball_tree','kd_tree','brute']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_neighbors' is {} for the optimal model.\".format(clf.get_params()['n_neighbors']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a BaggingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0480 seconds\n",
      "Made predictions in 0.0030 seconds.\n",
      "F1 score for training set: 0.9440.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for test set: 0.7597.\n"
     ]
    }
   ],
   "source": [
    "#tuning BaggingClassifier\n",
    "clf_A = BaggingClassifier(tree.DecisionTreeClassifier(), n_estimators=10, max_samples=0.5, max_features=0.5)\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0070 seconds.\n",
      "Tuned model has a training F1 score of 0.9785.\n",
      "Made predictions in 0.0050 seconds.\n",
      "Tuned model has a testing F1 score of 0.8148.\n",
      "Parameter 'n_estimators' is 50 for the optimal model.\n",
      "{'max_features': 1.0, 'max_samples': 0.5, 'bootstrap': 'True', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "param_range= [0.5,1.0]\n",
    "parameters = [{'n_estimators': estimators_range,'max_samples':param_range,'max_features':param_range,'bootstrap':['True','False']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = BaggingClassifier(tree.DecisionTreeClassifier(),random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=2)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.1550 seconds.\n",
      "Tuned model has a training F1 score of 0.8391.\n",
      "Made predictions in 0.0590 seconds.\n",
      "Tuned model has a testing F1 score of 0.7867.\n",
      "Parameter 'n_estimators' is 50 for the optimal model.\n",
      "{'max_features': 0.5, 'max_samples': 0.5, 'bootstrap': 'True', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "param_range= [0.5, 1.0]\n",
    "parameters = [{'n_estimators': estimators_range,'max_samples':param_range,'max_features':param_range,'bootstrap':['True','False']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = BaggingClassifier(neighbors.KNeighborsClassifier(),random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0230 seconds.\n",
      "Tuned model has a training F1 score of 0.8209.\n",
      "Made predictions in 0.0070 seconds.\n",
      "Tuned model has a testing F1 score of 0.7867.\n",
      "Parameter 'n_estimators' is 50 for the optimal model.\n",
      "{'max_features': 0.5, 'max_samples': 0.5, 'bootstrap': 'True', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "param_range= [0.5, 1.0]\n",
    "parameters = [{'n_estimators': estimators_range,'max_samples':param_range,'max_features':param_range,'bootstrap':['True','False']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = BaggingClassifier(linear_model.SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=100),random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a training F1 score of 0.8313.\n",
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a testing F1 score of 0.7945.\n",
      "Parameter 'n_estimators' is 5 for the optimal model.\n",
      "{'max_features': 1.0, 'max_samples': 1.0, 'bootstrap': 'True', 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "param_range= [0.5, 1.0]\n",
    "parameters = [{'n_estimators': estimators_range,'max_samples':param_range,'max_features':param_range,'bootstrap':['True','False']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = BaggingClassifier(linear_model.LogisticRegression(penalty='l1', C=0.1, tol=1e-5, solver='liblinear'),random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0450 seconds.\n",
      "Tuned model has a training F1 score of 0.9035.\n",
      "Made predictions in 0.0150 seconds.\n",
      "Tuned model has a testing F1 score of 0.7919.\n",
      "Parameter 'n_estimators' is 10 for the optimal model.\n",
      "{'max_features': 0.5, 'max_samples': 1.0, 'bootstrap': 'True', 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "param_range= [0.5, 1.0]\n",
    "parameters = [{'n_estimators': estimators_range,'max_samples':param_range,'max_features':param_range,'bootstrap':['True','False']}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = BaggingClassifier(svm.SVC(kernel='rbf', C=1, gamma=0.1, decision_function_shape='ovo'),random_state=1)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a training F1 score of 0.8340.\n",
      "Made predictions in 0.0010 seconds.\n",
      "Tuned model has a testing F1 score of 0.7785.\n",
      "Parameter 'n_estimators' is 10 for the optimal model.\n",
      "{'n_estimators': 10, 'max_leaf_nodes': 6, 'criterion': 'entropy', 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "estimators_range= [5,10,50]\n",
    "parameters = [{'n_estimators': estimators_range,'criterion':['gini','entropy'],'max_depth':np.arange(1, 11),'max_leaf_nodes':np.arange(2, 11)}\n",
    "             ]\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "from sklearn.metrics import f1_score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=f1_scorer, cv=10, n_jobs=2)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(clf.get_params()['n_estimators']))\n",
    "print (grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
